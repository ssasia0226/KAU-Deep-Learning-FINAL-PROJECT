{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "석사trasnform.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0PM9VsofK4k",
        "outputId": "c7fa6b8a-8b72-479e-c614-20eb51f18d84"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import matplotlib\n",
        "if os.environ.get('DISPLAY','') == '':\n",
        "    print('no display found. Using non-interactive Agg backend')\n",
        "    matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt \n",
        "# The squash function specified in Dynamic Routing Between Capsules\n",
        "# x: input tensor \n",
        "def squash(x, dim=-1):\n",
        "  norm_squared = (x ** 2).sum(dim, keepdim=True)\n",
        "  part1 = norm_squared / (1 +  norm_squared)\n",
        "  part2 = x / torch.sqrt(norm_squared+ 1e-16)\n",
        "\n",
        "  output = part1 * part2 \n",
        "  return output\n",
        "\n",
        "def weights_init_xavier(m):\n",
        "    classname = m.__class__.__name__\n",
        "    ignore_modules = [\n",
        "        \"SmallNorbConvReconstructionModule\",\n",
        "        \"ConvReconstructionModule\",\n",
        "        \"ConvLayer\"\n",
        "    ]\n",
        "    \n",
        "    if classname.find('Conv') != -1 and classname not in ignore_modules:\n",
        "        nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname == 'ClassCapsules': \n",
        "        nn.init.xavier_normal_(m.W.data, gain=0.002)\n",
        "        nn.init.xavier_normal_(m.bias.data, gain=0.002)\n",
        "        \n",
        "        \n",
        "def initialize_weights(capsnet):\n",
        "    capsnet.apply(weights_init_xavier)\n",
        "    \n",
        "def denormalize(image):\n",
        "    image = image - image.min()\n",
        "    image = image / image.max()\n",
        "    return image\n",
        "  \n",
        "    \n",
        "def get_path(SAVE_DIR, filename):\n",
        "    if not os.path.isdir(SAVE_DIR):\n",
        "        os.makedirs(SAVE_DIR)\n",
        "    path = os.path.join(SAVE_DIR, filename)\n",
        "    return path\n",
        "    \n",
        "def save_images(SAVE_DIR, filename, images, reconstructions, num_images = 100, imsize=28):\n",
        "    if len(images) < num_images or len(reconstructions) < num_images:\n",
        "        print(\"Not enough images to save.\")\n",
        "        return\n",
        "\n",
        "    big_image = np.ones((imsize*10, imsize*20+1))\n",
        "    images = denormalize(images).view(-1, imsize, imsize)\n",
        "    reconstructions = denormalize(reconstructions).view(-1, imsize, imsize)\n",
        "    images = images.data.cpu().numpy()\n",
        "    reconstructions = reconstructions.data.cpu().numpy()\n",
        "    for i in range(num_images):\n",
        "        image = images[i]\n",
        "        rec = reconstructions[i]\n",
        "        j = i % 10\n",
        "        i = i // 10\n",
        "        big_image[i*imsize:(i+1)*imsize, j*imsize:(j+1)*imsize] = image\n",
        "        j += 10\n",
        "        big_image[i*imsize:(i+1)*imsize, j*imsize+1:(j+1)*imsize+1] = rec\n",
        "\n",
        "    path = get_path(SAVE_DIR, filename)\n",
        "    plt.imsave(path, big_image, cmap=\"gray\")\n",
        "\n",
        "def save_images_cifar10(SAVE_DIR, filename, images, reconstructions, num_images = 100):\n",
        "    if len(images) < num_images or len(reconstructions) < num_images:\n",
        "        print(\"Not enough images to save.\")\n",
        "        return\n",
        "\n",
        "    big_image = np.ones((3,32*10, 32*20+1))\n",
        "    #print('Images : ',big_image.T.shape,',',reconstructions.size())\n",
        "    images = denormalize(images).view(-1, 3 ,32, 32)\n",
        "    reconstructions = denormalize(reconstructions).view(-1, 3 ,32, 32)\n",
        "    images = images.data.cpu().numpy()\n",
        "    reconstructions = reconstructions.data.cpu().numpy()\n",
        "    for i in range(num_images):\n",
        "        image = images[i]\n",
        "        rec = reconstructions[i]\n",
        "        j = i % 10\n",
        "        i = i // 10\n",
        "        big_image[:,i*32:(i+1)*32, j*32:(j+1)*32] = image\n",
        "        j += 10\n",
        "        big_image[:,i*32:(i+1)*32, j*32+1:(j+1)*32+1] = rec\n",
        "\n",
        "    path = get_path(SAVE_DIR, filename)\n",
        "    plt.imsave(path, big_image.T)\n",
        "\n",
        "\n",
        "def squash(x, dim=-1):\n",
        "  norm_squared = (x ** 2).sum(dim, keepdim=True)\n",
        "  part1 = norm_squared / (1 +  norm_squared)\n",
        "  part2 = x / torch.sqrt(norm_squared+ 1e-16)\n",
        "\n",
        "  output = part1 * part2 \n",
        "  return output"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no display found. Using non-interactive Agg backend\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC0K6CYIe1wC"
      },
      "source": [
        "# Loader taken from https://github.com/mavanb/vision/blob/448fac0f38cab35a387666d553b9d5e4eec4c5e6/torchvision/datasets/utils.py\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import errno\n",
        "import struct\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.datasets.utils import download_url, check_integrity\n",
        "\n",
        "\n",
        "class SmallNORB(data.Dataset):\n",
        "    \"\"\"`MNIST <https://cs.nyu.edu/~ylclab/data/norb-v1.0-small//>`_ Dataset.\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where processed folder and\n",
        "            and  raw folder exist.\n",
        "        train (bool, optional): If True, creates dataset from the training files,\n",
        "            otherwise from the test files.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If the dataset is already processed, it is not processed\n",
        "            and downloaded again. If dataset is only already downloaded, it is not\n",
        "            downloaded again.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        info_transform (callable, optional): A function/transform that takes in the\n",
        "            info and transforms it.\n",
        "        mode (string, optional): Denotes how the images in the data files are returned. Possible values:\n",
        "            - all (default): both left and right are included separately.\n",
        "            - stereo: left and right images are included as corresponding pairs.\n",
        "            - left: only the left images are included.\n",
        "            - right: only the right images are included.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_root = \"https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/\"\n",
        "    data_files = {\n",
        "        'train': {\n",
        "            'dat': {\n",
        "                \"name\": 'smallnorb-5x46789x9x18x6x2x96x96-training-dat.mat',\n",
        "                \"md5_gz\": \"66054832f9accfe74a0f4c36a75bc0a2\",\n",
        "                \"md5\": \"8138a0902307b32dfa0025a36dfa45ec\"\n",
        "            },\n",
        "            'info': {\n",
        "                \"name\": 'smallnorb-5x46789x9x18x6x2x96x96-training-info.mat',\n",
        "                \"md5_gz\": \"51dee1210a742582ff607dfd94e332e3\",\n",
        "                \"md5\": \"19faee774120001fc7e17980d6960451\"\n",
        "            },\n",
        "            'cat': {\n",
        "                \"name\": 'smallnorb-5x46789x9x18x6x2x96x96-training-cat.mat',\n",
        "                \"md5_gz\": \"23c8b86101fbf0904a000b43d3ed2fd9\",\n",
        "                \"md5\": \"fd5120d3f770ad57ebe620eb61a0b633\"\n",
        "            },\n",
        "        },\n",
        "        'test': {\n",
        "            'dat': {\n",
        "                \"name\": 'smallnorb-5x01235x9x18x6x2x96x96-testing-dat.mat',\n",
        "                \"md5_gz\": \"e4ad715691ed5a3a5f138751a4ceb071\",\n",
        "                \"md5\": \"e9920b7f7b2869a8f1a12e945b2c166c\"\n",
        "            },\n",
        "            'info': {\n",
        "                \"name\": 'smallnorb-5x01235x9x18x6x2x96x96-testing-info.mat',\n",
        "                \"md5_gz\": \"a9454f3864d7fd4bb3ea7fc3eb84924e\",\n",
        "                \"md5\": \"7c5b871cc69dcadec1bf6a18141f5edc\"\n",
        "            },\n",
        "            'cat': {\n",
        "                \"name\": 'smallnorb-5x01235x9x18x6x2x96x96-testing-cat.mat',\n",
        "                \"md5_gz\": \"5aa791cd7e6016cf957ce9bdb93b8603\",\n",
        "                \"md5\": \"fd5120d3f770ad57ebe620eb61a0b633\"\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    raw_folder = 'raw'\n",
        "    processed_folder = 'processed'\n",
        "    train_image_file = 'train_img'\n",
        "    train_label_file = 'train_label'\n",
        "    train_info_file = 'train_info'\n",
        "    test_image_file = 'test_img'\n",
        "    test_label_file = 'test_label'\n",
        "    test_info_file = 'test_info'\n",
        "    extension = '.pt'\n",
        "\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, info_transform=None, download=False,\n",
        "                 mode=\"all\"):\n",
        "\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.info_transform = info_transform\n",
        "        self.train = train  # training set or test set\n",
        "        self.mode = mode\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' You can use download=True to download it')\n",
        "\n",
        "        # load test or train set\n",
        "        image_file = self.train_image_file if self.train else self.test_image_file\n",
        "        label_file = self.train_label_file if self.train else self.test_label_file\n",
        "        info_file = self.train_info_file if self.train else self.test_info_file\n",
        "        # load labels\n",
        "        self.labels = self._load(label_file)\n",
        "        # load info files\n",
        "        self.infos = self._load(info_file)\n",
        "\n",
        "        # load right set\n",
        "        if self.mode == \"left\":\n",
        "            self.data = self._load(\"{}_left\".format(image_file))\n",
        "\n",
        "        # load left set\n",
        "        elif self.mode == \"right\":\n",
        "            self.data = self._load(\"{}_right\".format(image_file))\n",
        "\n",
        "        elif self.mode == \"all\" or self.mode == \"stereo\":\n",
        "            left_data = self._load(\"{}_left\".format(image_file))\n",
        "            right_data = self._load(\"{}_right\".format(image_file))\n",
        "            # load stereo\n",
        "            if self.mode == \"stereo\":\n",
        "                self.data = torch.stack((left_data, right_data), dim=1)\n",
        "    \n",
        "            # load all\n",
        "            else:\n",
        "                self.data = torch.cat((left_data, right_data), dim=0)\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            mode ``all'', ``left'', ``right'':\n",
        "                tuple: (image, target, info)\n",
        "            mode ``stereo'':\n",
        "                tuple: (image left, image right, target, info)\n",
        "        \"\"\"\n",
        "        target = self.labels[index % 24300] if self.mode is \"all\" else self.labels[index]\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        info = self.infos[index % 24300] if self.mode is \"all\" else self.infos[index]\n",
        "        if self.info_transform is not None:\n",
        "            info = self.info_transform(info)\n",
        "\n",
        "        if self.mode == \"stereo\":\n",
        "            img_left = self._transform(self.data[index, 0])\n",
        "            img_right = self._transform(self.data[index, 1])\n",
        "            return img_left, img_right, target, info\n",
        "\n",
        "        img = self._transform(self.data[index])\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _transform(self, img):\n",
        "        # doing this so that it is consistent with all other data sets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img.numpy(), mode='L')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def _load(self, file_name):\n",
        "        return torch.load(os.path.join(self.root, self.processed_folder, file_name + self.extension))\n",
        "\n",
        "    def _save(self, file, file_name):\n",
        "        with open(os.path.join(self.root, self.processed_folder, file_name + self.extension), 'wb') as f:\n",
        "            torch.save(file, f)\n",
        "\n",
        "    def _check_exists(self):\n",
        "        \"\"\" Check if processed files exists.\"\"\"\n",
        "        files = (\n",
        "            \"{}_left\".format(self.train_image_file),\n",
        "            \"{}_right\".format(self.train_image_file),\n",
        "            \"{}_left\".format(self.test_image_file),\n",
        "            \"{}_right\".format(self.test_image_file),\n",
        "            self.test_label_file,\n",
        "            self.train_label_file\n",
        "        )\n",
        "        fpaths = [os.path.exists(os.path.join(self.root, self.processed_folder, f + self.extension)) for f in files]\n",
        "        return False not in fpaths\n",
        "\n",
        "    def _flat_data_files(self):\n",
        "        return [j for i in self.data_files.values() for j in list(i.values())]\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        \"\"\"Check if unpacked files have correct md5 sum.\"\"\"\n",
        "        root = self.root\n",
        "        for file_dict in self._flat_data_files():\n",
        "            filename = file_dict[\"name\"]\n",
        "            md5 = file_dict[\"md5\"]\n",
        "            fpath = os.path.join(root, self.raw_folder, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def download(self):\n",
        "        \"\"\"Download the SmallNORB data if it doesn't exist in processed_folder already.\"\"\"\n",
        "        import gzip\n",
        "\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        # check if already extracted and verified\n",
        "        if self._check_integrity():\n",
        "            print('Files already downloaded and verified')\n",
        "        else:\n",
        "            # download and extract\n",
        "            for file_dict in self._flat_data_files():\n",
        "                url = self.dataset_root + file_dict[\"name\"] + '.gz'\n",
        "                filename = file_dict[\"name\"]\n",
        "                gz_filename = filename + '.gz'\n",
        "                md5 = file_dict[\"md5_gz\"]\n",
        "                fpath = os.path.join(self.root, self.raw_folder, filename)\n",
        "                gz_fpath = fpath + '.gz'\n",
        "\n",
        "                # download if compressed file not exists and verified\n",
        "                download_url(url, os.path.join(self.root, self.raw_folder), gz_filename, md5)\n",
        "\n",
        "                print('# Extracting data {}\\n'.format(filename))\n",
        "\n",
        "                with open(fpath, 'wb') as out_f, \\\n",
        "                        gzip.GzipFile(gz_fpath) as zip_f:\n",
        "                    out_f.write(zip_f.read())\n",
        "\n",
        "                os.unlink(gz_fpath)\n",
        "\n",
        "        # process and save as torch files\n",
        "        print('Processing...')\n",
        "\n",
        "        # create processed folder\n",
        "        try:\n",
        "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
        "        except OSError as e:\n",
        "            if e.errno == errno.EEXIST:\n",
        "                pass\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        # read train files\n",
        "        left_train_img, right_train_img = self._read_image_file(self.data_files[\"train\"][\"dat\"][\"name\"])\n",
        "        train_info = self._read_info_file(self.data_files[\"train\"][\"info\"][\"name\"])\n",
        "        train_label = self._read_label_file(self.data_files[\"train\"][\"cat\"][\"name\"])\n",
        "\n",
        "        # read test files\n",
        "        left_test_img, right_test_img = self._read_image_file(self.data_files[\"test\"][\"dat\"][\"name\"])\n",
        "        test_info = self._read_info_file(self.data_files[\"test\"][\"info\"][\"name\"])\n",
        "        test_label = self._read_label_file(self.data_files[\"test\"][\"cat\"][\"name\"])\n",
        "\n",
        "        # save training files\n",
        "        self._save(left_train_img, \"{}_left\".format(self.train_image_file))\n",
        "        self._save(right_train_img, \"{}_right\".format(self.train_image_file))\n",
        "        self._save(train_label, self.train_label_file)\n",
        "        self._save(train_info, self.train_info_file)\n",
        "\n",
        "        # save test files\n",
        "        self._save(left_test_img, \"{}_left\".format(self.test_image_file))\n",
        "        self._save(right_test_img, \"{}_right\".format(self.test_image_file))\n",
        "        self._save(test_label, self.test_label_file)\n",
        "        self._save(test_info, self.test_info_file)\n",
        "\n",
        "        print('Done!')\n",
        "\n",
        "    @staticmethod\n",
        "    def _parse_header(file_pointer):\n",
        "        # Read magic number and ignore\n",
        "        struct.unpack('<BBBB', file_pointer.read(4))  # '<' is little endian)\n",
        "\n",
        "        # Read dimensions\n",
        "        dimensions = []\n",
        "        num_dims, = struct.unpack('<i', file_pointer.read(4))  # '<' is little endian)\n",
        "        for _ in range(num_dims):\n",
        "            dimensions.extend(struct.unpack('<i', file_pointer.read(4)))\n",
        "\n",
        "        return dimensions\n",
        "\n",
        "    def _read_image_file(self, file_name):\n",
        "        fpath = os.path.join(self.root, self.raw_folder, file_name)\n",
        "        with open(fpath, mode='rb') as f:\n",
        "            dimensions = self._parse_header(f)\n",
        "            assert dimensions == [24300, 2, 96, 96]\n",
        "            num_samples, _, height, width = dimensions\n",
        "\n",
        "            left_samples = np.zeros(shape=(num_samples, height, width), dtype=np.uint8)\n",
        "            right_samples = np.zeros(shape=(num_samples, height, width), dtype=np.uint8)\n",
        "\n",
        "            for i in range(num_samples):\n",
        "\n",
        "                # left and right images stored in pairs, left first\n",
        "                left_samples[i, :, :] = self._read_image(f, height, width)\n",
        "                right_samples[i, :, :] = self._read_image(f, height, width)\n",
        "\n",
        "        return torch.ByteTensor(left_samples), torch.ByteTensor(right_samples)\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_image(file_pointer, height, width):\n",
        "        \"\"\"Read raw image data and restore shape as appropriate. \"\"\"\n",
        "        image = struct.unpack('<' + height * width * 'B', file_pointer.read(height * width))\n",
        "        image = np.uint8(np.reshape(image, newshape=(height, width)))\n",
        "        return image\n",
        "\n",
        "    def _read_label_file(self, file_name):\n",
        "        fpath = os.path.join(self.root, self.raw_folder, file_name)\n",
        "        with open(fpath, mode='rb') as f:\n",
        "            dimensions = self._parse_header(f)\n",
        "            assert dimensions == [24300]\n",
        "            num_samples = dimensions[0]\n",
        "\n",
        "            struct.unpack('<BBBB', f.read(4))  # ignore this integer\n",
        "            struct.unpack('<BBBB', f.read(4))  # ignore this integer\n",
        "\n",
        "            labels = np.zeros(shape=num_samples, dtype=np.int32)\n",
        "            for i in range(num_samples):\n",
        "                category, = struct.unpack('<i', f.read(4))\n",
        "                labels[i] = category\n",
        "            return torch.LongTensor(labels)\n",
        "\n",
        "    def _read_info_file(self, file_name):\n",
        "        fpath = os.path.join(self.root, self.raw_folder, file_name)\n",
        "        with open(fpath, mode='rb') as f:\n",
        "\n",
        "            dimensions = self._parse_header(f)\n",
        "            assert dimensions == [24300, 4]\n",
        "            num_samples, num_info = dimensions\n",
        "\n",
        "            struct.unpack('<BBBB', f.read(4))  # ignore this integer\n",
        "\n",
        "            infos = np.zeros(shape=(num_samples, num_info), dtype=np.int32)\n",
        "\n",
        "            for r in range(num_samples):\n",
        "                for c in range(num_info):\n",
        "                    info, = struct.unpack('<i', f.read(4))\n",
        "                    infos[r, c] = info\n",
        "\n",
        "        return torch.LongTensor(infos)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKBxlVcesQq"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "USE_GPU=True\n",
        "\n",
        "def routing_algorithm(x, weight, bias, routing_iterations):\n",
        "    \"\"\"\n",
        "    x: [batch_size, num_capsules_in, capsule_dim]\n",
        "    weight: [1,num_capsules_in,num_capsules_out,out_channels,in_channels]\n",
        "    bias: [1,1, num_capsules_out, out_channels]\n",
        "    \"\"\"\n",
        "    num_capsules_in = x.shape[1]\n",
        "    num_capsules_out = weight.shape[2]\n",
        "    batch_size = x.size(0)\n",
        "    \n",
        "    x = x.unsqueeze(2).unsqueeze(4)\n",
        "\n",
        "    #[batch_size, 32*6*6, 10, 16]\n",
        "    u_hat = torch.matmul(weight, x).squeeze()\n",
        "\n",
        "    b_ij = Variable(x.new(batch_size, num_capsules_in, num_capsules_out, 1).zero_())\n",
        "\n",
        "\n",
        "    for it in range(routing_iterations):\n",
        "      c_ij = functional.softmax(b_ij, dim=2)\n",
        "\n",
        "      # [batch_size, 1, num_classes, capsule_size]\n",
        "      s_j = (c_ij * u_hat).sum(dim=1, keepdim=True) + bias\n",
        "      # [batch_size, 1, num_capsules, out_channels]\n",
        "      v_j = squash(s_j, dim=-1)\n",
        "      \n",
        "      if it < routing_iterations - 1: \n",
        "        # [batch-size, 32*6*6, 10, 1]\n",
        "        delta = (u_hat * v_j).sum(dim=-1, keepdim=True)\n",
        "        b_ij = b_ij + delta\n",
        "    \n",
        "    return v_j.squeeze()\n",
        "\n",
        "# First Convolutional Layer\n",
        "class ConvLayer(nn.Module):\n",
        "  def __init__(self, \n",
        "               in_channels=1, \n",
        "               out_channels=256, \n",
        "               kernel_size=9,\n",
        "               batchnorm=False):\n",
        "    super(ConvLayer, self).__init__()\n",
        "    \n",
        "    if batchnorm:\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels,\n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    else:\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels,\n",
        "                              out_channels=out_channels,\n",
        "                              kernel_size=kernel_size,\n",
        "                              stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "  def forward(self, x):\n",
        "    output = self.conv(x)\n",
        "    return output\n",
        "\n",
        "class PrimaryCapules(nn.Module):\n",
        "  \n",
        "  def __init__(self, \n",
        "               num_capsules=32, \n",
        "               in_channels=256, \n",
        "               out_channels=8, \n",
        "               kernel_size=9,\n",
        "               primary_caps_gridsize=6,\n",
        "               batchnorm=False):\n",
        "\n",
        "    super(PrimaryCapules, self).__init__()\n",
        "    self.gridsize = primary_caps_gridsize\n",
        "    self.num_capsules = num_capsules\n",
        "    if batchnorm:\n",
        "        self.capsules = nn.ModuleList([\n",
        "          nn.Sequential(\n",
        "          nn.Conv2d(in_channels=in_channels,\n",
        "                    out_channels=num_capsules,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=2,\n",
        "                    padding=0),\n",
        "          nn.BatchNorm2d(num_capsules)\n",
        "          )\n",
        "           for i in range(out_channels)\n",
        "        ])\n",
        "    else:\n",
        "        self.capsules = nn.ModuleList([\n",
        "          nn.Sequential(\n",
        "          nn.Conv2d(in_channels=in_channels,\n",
        "                    out_channels=num_capsules,\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=2,\n",
        "                    padding=0),\n",
        "\n",
        "          )\n",
        "           for i in range(out_channels)\n",
        "        ])\n",
        "  \n",
        "  def forward(self, x):\n",
        "    output = [caps(x) for caps in self.capsules]\n",
        "    output = torch.stack(output, dim=1)\n",
        "    output = output.view(x.size(0), self.num_capsules*(self.gridsize)*(self.gridsize), -1)\n",
        "    \n",
        "    return squash(output)\n",
        "\n",
        "\n",
        "class ClassCapsules(nn.Module):\n",
        "  \n",
        "  def __init__(self, \n",
        "               num_capsules=10,\n",
        "               num_routes = 32*6*6,\n",
        "               in_channels=8,\n",
        "               out_channels=16,\n",
        "               routing_iterations=3,\n",
        "               leaky=False):\n",
        "    super(ClassCapsules, self).__init__()\n",
        "    \n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.num_routes = num_routes\n",
        "    self.num_capsules = num_capsules\n",
        "    self.routing_iterations = routing_iterations\n",
        "    \n",
        "    self.W = nn.Parameter(torch.rand(1,num_routes,num_capsules,out_channels,in_channels))\n",
        "    self.bias = nn.Parameter(torch.rand(1,1, num_capsules, out_channels))\n",
        "\n",
        "\n",
        "  # [batch_size, 10, 16, 1]\n",
        "  def forward(self, x):\n",
        "    v_j = routing_algorithm(x, self.W, self.bias, self.routing_iterations)\n",
        "    return v_j.unsqueeze(-1)\n",
        "\n",
        "\n",
        "class ReconstructionModule(nn.Module):\n",
        "  def __init__(self, capsule_size=16, num_capsules=10, imsize=28,img_channel=1, batchnorm=False):\n",
        "    super(ReconstructionModule, self).__init__()\n",
        "    \n",
        "    self.num_capsules = num_capsules\n",
        "    self.capsule_size = capsule_size\n",
        "    self.imsize = imsize\n",
        "    self.img_channel = img_channel\n",
        "    if batchnorm:\n",
        "        self.decoder = nn.Sequential(\n",
        "              nn.Linear(capsule_size*num_capsules, 512),\n",
        "              nn.BatchNorm1d(512),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(512, 1024),        \n",
        "              nn.BatchNorm1d(1024),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(1024, imsize*imsize*img_channel),\n",
        "              nn.Sigmoid()\n",
        "        )\n",
        "    else:\n",
        "        self.decoder = nn.Sequential(\n",
        "              nn.Linear(capsule_size*num_capsules, 512),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(512, 1024),        \n",
        "              nn.ReLU(),\n",
        "              nn.Linear(1024, imsize*imsize*img_channel),\n",
        "              nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "  def forward(self, x, target=None):\n",
        "    batch_size = x.size(0)\n",
        "    if target is None:\n",
        "      classes = torch.norm(x, dim=2)\n",
        "      max_length_indices = classes.max(dim=1)[1].squeeze()\n",
        "    else:\n",
        "      max_length_indices = target.max(dim=1)[1]\n",
        "    \n",
        "    masked = Variable(x.new_tensor(torch.eye(self.num_capsules)))\n",
        "    masked=masked.cuda()\n",
        "    masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
        "    decoder_input = (x * masked[:, :, None, None]).view(batch_size, -1)\n",
        "\n",
        "    reconstructions = self.decoder(decoder_input)\n",
        "    reconstructions = reconstructions.view(-1, self.img_channel, self.imsize, self.imsize)\n",
        "    return reconstructions, masked\n",
        "\n",
        "class ConvReconstructionModule(nn.Module):\n",
        "  def __init__(self, num_capsules=10, capsule_size=16, imsize=28,img_channels=1, batchnorm=False):\n",
        "    super(ConvReconstructionModule, self).__init__()\n",
        "    self.num_capsules = num_capsules\n",
        "    self.capsule_size = capsule_size\n",
        "    self.imsize = imsize\n",
        "    self.img_channels = img_channels\n",
        "    self.grid_size = 6\n",
        "    if batchnorm:\n",
        "      self.FC = nn.Sequential(\n",
        "        nn.Linear(capsule_size * num_capsules, num_capsules * (self.grid_size)**2 ),\n",
        "        nn.BatchNorm1d(num_capsules * self.grid_size**2),\n",
        "        nn.ReLU()\n",
        "      )\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.ConvTranspose2d(in_channels=self.num_capsules, out_channels=32, kernel_size=9, stride=2),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=9, stride=1),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=2, stride=1),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "    else:\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(capsule_size * num_capsules, num_capsules *(self.grid_size**2) ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.ConvTranspose2d(in_channels=self.num_capsules, out_channels=32, kernel_size=9, stride=2),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=9, stride=1),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=2, stride=1),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "  def forward(self, x, target=None):\n",
        "    batch_size = x.size(0)\n",
        "    if target is None:\n",
        "      classes = torch.norm(x, dim=2)\n",
        "      max_length_indices = classes.max(dim=1)[1].squeeze()\n",
        "    else:\n",
        "      max_length_indices = target.max(dim=1)[1]\n",
        "    \n",
        "    masked = x.new_tensor(torch.eye(self.num_capsules))\n",
        "    masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
        "\n",
        "    decoder_input = (x * masked[:, :, None, None]).view(batch_size, -1)\n",
        "    decoder_input = self.FC(decoder_input)\n",
        "    decoder_input = decoder_input.view(batch_size,self.num_capsules, self.grid_size, self.grid_size)\n",
        "    reconstructions = self.decoder(decoder_input)\n",
        "    reconstructions = reconstructions.view(-1, self.img_channels, self.imsize, self.imsize)\n",
        "    \n",
        "    return reconstructions, masked\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SmallNorbConvReconstructionModule(nn.Module):\n",
        "  def __init__(self, num_capsules=10, capsule_size=16, imsize=28,img_channels=1, batchnorm=False):\n",
        "    super(SmallNorbConvReconstructionModule, self).__init__()\n",
        "    self.num_capsules = num_capsules\n",
        "    self.capsule_size = capsule_size\n",
        "    self.imsize = imsize\n",
        "    self.img_channels = img_channels\n",
        "    \n",
        "    self.grid_size = 4\n",
        "    \n",
        "    if batchnorm:\n",
        "      self.FC = nn.Sequential(\n",
        "            nn.Linear(capsule_size * num_capsules, num_capsules *self.grid_size*self.grid_size),\n",
        "            nn.BatchNorm1d(num_capsules * self.grid_size**2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.ConvTranspose2d(in_channels=num_capsules, out_channels=32, kernel_size=9, stride=2),\n",
        "          nn.BatchNorm2d(32),            \n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=9, stride=1),\n",
        "          nn.BatchNorm2d(64),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=9, stride=1),\n",
        "          nn.BatchNorm2d(128),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=2, stride=1),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "    else:\n",
        "        self.FC = nn.Sequential(\n",
        "            nn.Linear(capsule_size * num_capsules, num_capsules *(self.grid_size**2) ),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "          nn.ConvTranspose2d(in_channels=num_capsules, out_channels=32, kernel_size=9, stride=2),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=9, stride=1),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=9, stride=1),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=2, stride=1),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "  def forward(self, x, target=None):\n",
        "    batch_size = x.size(0)\n",
        "    if target is None:\n",
        "      classes = torch.norm(x, dim=2)\n",
        "      max_length_indices = classes.max(dim=1)[1].squeeze()\n",
        "    else:\n",
        "      max_length_indices = target.max(dim=1)[1]\n",
        "    masked = Variable(x.new_tensor(torch.eye(self.num_capsules)))\n",
        "    masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
        "\n",
        "    decoder_input = (x * masked[:, :, None, None]).view(batch_size, -1)\n",
        "    decoder_input = self.FC(decoder_input)\n",
        "    decoder_input = decoder_input.view(batch_size,self.num_capsules, self.grid_size, self.grid_size)\n",
        "    reconstructions = self.decoder(decoder_input)\n",
        "    reconstructions = reconstructions.view(-1, self.img_channels, self.imsize, self.imsize)\n",
        "    \n",
        "    return reconstructions, masked\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CapsNet(nn.Module):\n",
        "  \n",
        "  def __init__(self,\n",
        "               reconstruction_type = \"FC\",\n",
        "               imsize=28,\n",
        "               num_classes=10,\n",
        "               routing_iterations=3,\n",
        "               primary_caps_gridsize=6,\n",
        "               img_channels = 1,\n",
        "               batchnorm = False,\n",
        "               loss = \"L2\",\n",
        "               num_primary_capsules=32,\n",
        "               leaky_routing = False\n",
        "              ):\n",
        "    super(CapsNet, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    if leaky_routing:\n",
        "        num_classes += 1\n",
        "        self.num_classes += 1\n",
        "        \n",
        "    self.imsize=imsize\n",
        "    self.conv_layer = ConvLayer(in_channels=img_channels, batchnorm=batchnorm)\n",
        "    self.leaky_routing = leaky_routing\n",
        "\n",
        "    self.primary_capsules = PrimaryCapules(primary_caps_gridsize=primary_caps_gridsize,\n",
        "                                           batchnorm=batchnorm,\n",
        "                                           num_capsules = num_primary_capsules)\n",
        "    \n",
        "    self.digit_caps = ClassCapsules(num_capsules=num_classes,\n",
        "                                    num_routes=num_primary_capsules*primary_caps_gridsize*primary_caps_gridsize,\n",
        "                                    routing_iterations=routing_iterations,\n",
        "                                    leaky=leaky_routing)\n",
        "\n",
        "    if reconstruction_type == \"FC\":\n",
        "        self.decoder = ReconstructionModule(imsize=imsize,\n",
        "                                            num_capsules=num_classes,\n",
        "                                            img_channel=img_channels, \n",
        "                                            batchnorm=batchnorm)\n",
        "    elif reconstruction_type == \"Conv32\":\n",
        "        self.decoder = SmallNorbConvReconstructionModule(num_capsules=num_classes,\n",
        "                                                         imsize=imsize, \n",
        "                                                         img_channels=img_channels, \n",
        "                                                         batchnorm=batchnorm)            \n",
        "    else:\n",
        "        self.decoder = ConvReconstructionModule(num_capsules=num_classes,\n",
        "                                                imsize=imsize, \n",
        "                                                img_channels=img_channels,\n",
        "                                                batchnorm=batchnorm)\n",
        "    \n",
        "    if loss == \"L2\":\n",
        "        self.reconstruction_criterion = nn.MSELoss(reduction=\"none\")\n",
        "    if loss == \"L1\":\n",
        "        self.reconstruction_criterion = nn.L1Loss(reduction=\"none\")\n",
        "  \n",
        "  def forward(self, x, target=None):\n",
        "    output = self.conv_layer(x)\n",
        "    output = self.primary_capsules(output)\n",
        "    output = self.digit_caps(output)\n",
        "    reconstruction, masked = self.decoder(output, target)\n",
        "\n",
        "    return output, reconstruction, masked\n",
        "  \n",
        "  def loss(self, images, labels, capsule_output,  reconstruction, alpha):\n",
        "    marg_loss = self.margin_loss(capsule_output, labels)\n",
        "    rec_loss = self.reconstruction_loss(images, reconstruction)\n",
        "    total_loss = (marg_loss + alpha * rec_loss).mean()\n",
        "    return total_loss, rec_loss.mean(), marg_loss.mean()\n",
        "  \n",
        "  def margin_loss(self, x, labels):\n",
        "    batch_size = x.size(0)\n",
        "    v_c = torch.norm(x, dim=2, keepdim=True)\n",
        "    \n",
        "    left = functional.relu(0.9 - v_c).view(batch_size, -1) ** 2\n",
        "    right = functional.relu(v_c - 0.1).view(batch_size, -1) ** 2\n",
        "\n",
        "    loss = labels * left + 0.5 *(1-labels)*right\n",
        "    loss = loss.sum(dim=1)\n",
        "    return loss\n",
        "  \n",
        "  def reconstruction_loss(self, data, reconstructions):\n",
        "    batch_size = reconstructions.size(0)\n",
        "    reconstructions = reconstructions.view(batch_size, -1)\n",
        "    data = data.view(batch_size, -1)\n",
        "    loss = self.reconstruction_criterion(reconstructions, data)\n",
        "    loss = loss.sum(dim=1)\n",
        "    return loss\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7SFb7C_eyNx",
        "outputId": "8a5d9d09-3c48-410c-8e97-6ce2741c94e3"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "capsnet = CapsNet(reconstruction_type='FC',\n",
        "        imsize=32,\n",
        "        num_classes=5,\n",
        "        routing_iterations = 3,\n",
        "        primary_caps_gridsize=8,\n",
        "        num_primary_capsules=32,\n",
        "        batchnorm=True,\n",
        "        loss = 'L2',\n",
        "        leaky_routing=False)\n",
        "\n",
        "capsnet = capsnet.cuda()\n",
        "\n",
        "print(capsnet)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CapsNet(\n",
            "  (conv_layer): ConvLayer(\n",
            "    (conv): Sequential(\n",
            "      (0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (primary_capsules): PrimaryCapules(\n",
            "    (capsules): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): Sequential(\n",
            "        (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (6): Sequential(\n",
            "        (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (7): Sequential(\n",
            "        (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (digit_caps): ClassCapsules()\n",
            "  (decoder): ReconstructionModule(\n",
            "    (decoder): Sequential(\n",
            "      (0): Linear(in_features=80, out_features=512, bias=True)\n",
            "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (7): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (reconstruction_criterion): MSELoss()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfKROxkAeq8m",
        "scrolled": true
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "train_transform= transforms.Compose([\n",
        "    transforms.Resize(48),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ColorJitter(brightness=32./255, contrast=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.75,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(48),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.75,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "\n",
        "train_dataset = SmallNORB('./datasets/smallNORB/', train=True, download=True, transform=train_transform)\n",
        "test_dataset = SmallNORB('./datasets/smallNORB/', train=False, transform=test_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "capsnet = CapsNet(reconstruction_type='FC',\n",
        "        imsize=32,\n",
        "        num_classes=5,\n",
        "        routing_iterations = 3,\n",
        "        primary_caps_gridsize=8,\n",
        "        num_primary_capsules=32,\n",
        "        batchnorm=True,\n",
        "        loss = 'L2',\n",
        "        leaky_routing=False)\n",
        "\n",
        "initialize_weights(capsnet)\n",
        "capsnet = capsnet.cuda()\n",
        "\n",
        "new_model=capsnet\n",
        "\n",
        "# pruning\n",
        "\n",
        "for name, module in new_model.named_modules():\n",
        "    # 모든 2D-conv 층의 20% 연결에 대해 가지치기 기법을 적용\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # 모든 선형 층의 40% 연결에 대해 가지치기 기법을 적용\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "best_acc = 0\n",
        "optimizer = torch.optim.Adam(capsnet.parameters(), lr=0.001)\n",
        "\n",
        "def train(epoch):\n",
        "    capsnet.train()\n",
        "    train_correct = 0\n",
        "    total = 0\n",
        "    for batch, (data, target) in tqdm(list(enumerate(train_loader)), ascii=True, desc=\"Epoch{:3d}\".format(epoch)):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        target = torch.eye(5).cuda().index_select(dim=0, index=target)\n",
        "        optimizer.zero_grad()\n",
        "        capsule_output, reconstructions, _ = capsnet(data, target)\n",
        "        predictions = torch.norm(capsule_output.squeeze(), dim=2)\n",
        "        loss, rec_loss, marg_loss = capsnet.loss(data, target, capsule_output, reconstructions, 0.0005)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_correct += (target.max(dim=1)[1] == predictions.max(dim=1)[1]).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    print(\"acc = {}%\".format(train_correct/total))\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    capsnet.eval()\n",
        "    test_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_id, (data, target) in tqdm(list(enumerate(test_loader)), ascii=True, desc=\"Test {:3d}\".format(epoch)):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        target = torch.eye(5).cuda().index_select(dim=0, index=target)\n",
        "\n",
        "        capsule_output, reconstructions, predictions = capsnet(data)\n",
        "        data = denormalize(data)\n",
        "        loss, rec_loss, marg_loss = capsnet.loss(data, target, capsule_output, reconstructions, 0.0005)\n",
        "\n",
        "        test_correct += (target.max(dim=1)[1] == predictions.max(dim=1)[1]).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    print(\"acc = {}%\".format(test_correct/total))\n",
        "    acc = 100.*test_correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': capsnet.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/{}.pt'.format(str(acc)))\n",
        "        best_acc = acc\n",
        "\n",
        "for epoch in range(200):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOH7wfnULKuo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtJ3nSIVeQq6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn7jXT3qeQq6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}