{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsgLFYdiFITx",
    "outputId": "c92a7640-0cbb-43dd-9dfb-ae2a201c04d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "if os.environ.get('DISPLAY','') == '':\n",
    "    print('no display found. Using non-interactive Agg backend')\n",
    "    matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt \n",
    "# The squash function specified in Dynamic Routing Between Capsules\n",
    "# x: input tensor \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "def squash(x, dim=-1):\n",
    "  norm_squared = (x ** 2).sum(dim, keepdim=True)\n",
    "  part1 = norm_squared / (1 +  norm_squared)\n",
    "  part2 = x / torch.sqrt(norm_squared+ 1e-16)\n",
    "\n",
    "  output = part1 * part2 \n",
    "  return output\n",
    "\n",
    "\n",
    "def quantize(s, word, fraction):\n",
    "  shift = pow(2, fraction)\n",
    "  s = s * float(shift)\n",
    "  s = torch.round(s)\n",
    "  sat_p = pow(2, word-1)-1\n",
    "  sat_m = pow(2, word-1)*(-1)\n",
    "  s = torch.clamp(s, sat_m, sat_p)\n",
    "  #s = np.int8(s)\n",
    "  s = s / float(shift)\n",
    "  s= s.cuda().float()\n",
    "  return s\n",
    "\n",
    "def weights_init_xavier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    ignore_modules = [\n",
    "        \"SmallNorbConvReconstructionModule\",\n",
    "        \"ConvReconstructionModule\",\n",
    "        \"ConvLayer\"\n",
    "    ]\n",
    "    \n",
    "    if classname.find('Conv') != -1 and classname not in ignore_modules:\n",
    "        nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname == 'ClassCapsules': \n",
    "        nn.init.xavier_normal_(m.W.data, gain=0.002)\n",
    "        nn.init.xavier_normal_(m.bias.data, gain=0.002)\n",
    "        \n",
    "        \n",
    "def initialize_weights(capsnet):\n",
    "    capsnet.apply(weights_init_xavier)\n",
    "    \n",
    "def denormalize(image):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max()\n",
    "    return image\n",
    "  \n",
    "    \n",
    "def get_path(SAVE_DIR, filename):\n",
    "    if not os.path.isdir(SAVE_DIR):\n",
    "        os.makedirs(SAVE_DIR)\n",
    "    path = os.path.join(SAVE_DIR, filename)\n",
    "    return path\n",
    "    \n",
    "def save_images(SAVE_DIR, filename, images, reconstructions, num_images = 100, imsize=28):\n",
    "    if len(images) < num_images or len(reconstructions) < num_images:\n",
    "        print(\"Not enough images to save.\")\n",
    "        return\n",
    "\n",
    "    big_image = np.ones((imsize*10, imsize*20+1))\n",
    "    images = denormalize(images).view(-1, imsize, imsize)\n",
    "    reconstructions = denormalize(reconstructions).view(-1, imsize, imsize)\n",
    "    images = images.data.cpu().numpy()\n",
    "    reconstructions = reconstructions.data.cpu().numpy()\n",
    "    for i in range(num_images):\n",
    "        image = images[i]\n",
    "        rec = reconstructions[i]\n",
    "        j = i % 10\n",
    "        i = i // 10\n",
    "        big_image[i*imsize:(i+1)*imsize, j*imsize:(j+1)*imsize] = image\n",
    "        j += 10\n",
    "        big_image[i*imsize:(i+1)*imsize, j*imsize+1:(j+1)*imsize+1] = rec\n",
    "\n",
    "    path = get_path(SAVE_DIR, filename)\n",
    "    plt.imsave(path, big_image, cmap=\"gray\")\n",
    "\n",
    "def save_images_cifar10(SAVE_DIR, filename, images, reconstructions, num_images = 100):\n",
    "    if len(images) < num_images or len(reconstructions) < num_images:\n",
    "        print(\"Not enough images to save.\")\n",
    "        return\n",
    "\n",
    "    big_image = np.ones((3,32*10, 32*20+1))\n",
    "    #print('Images : ',big_image.T.shape,',',reconstructions.size())\n",
    "    images = denormalize(images).view(-1, 3 ,32, 32)\n",
    "    reconstructions = denormalize(reconstructions).view(-1, 3 ,32, 32)\n",
    "    images = images.data.cpu().numpy()\n",
    "    reconstructions = reconstructions.data.cpu().numpy()\n",
    "    for i in range(num_images):\n",
    "        image = images[i]\n",
    "        rec = reconstructions[i]\n",
    "        j = i % 10\n",
    "        i = i // 10\n",
    "        big_image[:,i*32:(i+1)*32, j*32:(j+1)*32] = image\n",
    "        j += 10\n",
    "        big_image[:,i*32:(i+1)*32, j*32+1:(j+1)*32+1] = rec\n",
    "\n",
    "    path = get_path(SAVE_DIR, filename)\n",
    "    plt.imsave(path, big_image.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NtBZynB5FNEm"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "USE_GPU=True\n",
    "\n",
    "def routing_algorithm(x, weight, bias, routing_iterations):\n",
    "    \"\"\"\n",
    "    x: [batch_size, num_capsules_in, capsule_dim]\n",
    "    weight: [1,num_capsules_in,num_capsules_out,out_channels,in_channels]\n",
    "    bias: [1,1, num_capsules_out, out_channels]\n",
    "    \"\"\"\n",
    "    num_capsules_in = x.shape[1]\n",
    "    num_capsules_out = weight.shape[2]\n",
    "    batch_size = x.size(0)\n",
    "    \n",
    "    x = x.unsqueeze(2).unsqueeze(4)\n",
    "    #weight = quantize(weight,8,7)\n",
    "    #bais = quantize(bias,8,7)\n",
    "    #[batch_size, 32*6*6, 10, 16]\n",
    "    u_hat = torch.matmul(weight, x).squeeze()\n",
    "    #u_hat = quantize(u_hat,8,7)\n",
    "    b_ij = Variable(x.new(batch_size, num_capsules_in, num_capsules_out, 1).zero_())\n",
    "\n",
    "\n",
    "    for it in range(routing_iterations):\n",
    "      c_ij = functional.softmax(b_ij, dim=2)\n",
    "      #c_ij = quantize(c_ij,8,7)\n",
    "      # [batch_size, 1, num_classes, capsule_size]\n",
    "      s_j = (c_ij * u_hat).sum(dim=1, keepdim=True) + bias\n",
    "      s_j = quantize(s_j,8,7)\n",
    "      # [batch_size, 1, num_capsules, out_channels]\n",
    "      v_j = squash(s_j, dim=-1)\n",
    "      if it < routing_iterations - 1: \n",
    "        # [batch-size, 32*6*6, 10, 1]\n",
    "        delta = (u_hat * v_j).sum(dim=-1, keepdim=True)\n",
    "        b_ij = b_ij + delta\n",
    "    #v_j = quantize(v_j,8,7)\n",
    "    return v_j.squeeze()\n",
    "\n",
    "# First Convolutional Layer\n",
    "class ConvLayer(nn.Module):\n",
    "  def __init__(self, \n",
    "               in_channels=1, \n",
    "               out_channels=256, \n",
    "               kernel_size=9,\n",
    "               batchnorm=False):\n",
    "    super(ConvLayer, self).__init__()\n",
    "    \n",
    "    if batchnorm:\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    else:\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "  def forward(self, x):\n",
    "    #x = quantize(x, 8, 7)\n",
    "    output = self.conv(x)\n",
    "    output = quantize(output, 8, 6)\n",
    "    return output\n",
    "\n",
    "class PrimaryCapules(nn.Module):\n",
    "  \n",
    "  def __init__(self, \n",
    "               num_capsules=32, \n",
    "               in_channels=256, \n",
    "               out_channels=8, \n",
    "               kernel_size=9,\n",
    "               primary_caps_gridsize=6,\n",
    "               batchnorm=False):\n",
    "\n",
    "    super(PrimaryCapules, self).__init__()\n",
    "    self.gridsize = primary_caps_gridsize\n",
    "    self.num_capsules = num_capsules\n",
    "    if batchnorm:\n",
    "        self.capsules = nn.ModuleList([\n",
    "          nn.Sequential(\n",
    "          nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=num_capsules,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=2,\n",
    "                    padding=0),\n",
    "          nn.BatchNorm2d(num_capsules)\n",
    "          )\n",
    "           for i in range(out_channels)\n",
    "        ])\n",
    "    else:\n",
    "        self.capsules = nn.ModuleList([\n",
    "          nn.Sequential(\n",
    "          nn.Conv2d(in_channels=in_channels,\n",
    "                    out_channels=num_capsules,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=2,\n",
    "                    padding=0),\n",
    "\n",
    "          )\n",
    "           for i in range(out_channels)\n",
    "        ])\n",
    "  \n",
    "  def forward(self, x):\n",
    "    output = [caps(x) for caps in self.capsules]\n",
    "    output = torch.stack(output, dim=1)\n",
    "    output = quantize(output, 8, 6)\n",
    "    output = output.view(x.size(0), self.num_capsules*(self.gridsize)*(self.gridsize), -1)\n",
    "    output = squash(output)\n",
    "    output = quantize(output, 8, 6)\n",
    "    return output\n",
    "\n",
    "\n",
    "class ClassCapsules(nn.Module):\n",
    "  \n",
    "  def __init__(self, \n",
    "               num_capsules=10,\n",
    "               num_routes = 32*6*6,\n",
    "               in_channels=8,\n",
    "               out_channels=16,\n",
    "               routing_iterations=3,\n",
    "               leaky=False):\n",
    "    super(ClassCapsules, self).__init__()\n",
    "    \n",
    "\n",
    "    self.in_channels = in_channels\n",
    "    self.num_routes = num_routes\n",
    "    self.num_capsules = num_capsules\n",
    "    self.routing_iterations = routing_iterations\n",
    "    \n",
    "    self.W = nn.Parameter(torch.rand(1,num_routes,num_capsules,out_channels,in_channels))\n",
    "    self.bias = nn.Parameter(torch.rand(1,1, num_capsules, out_channels))\n",
    "\n",
    "\n",
    "  # [batch_size, 10, 16, 1]\n",
    "  def forward(self, x):\n",
    "    v_j = routing_algorithm(x, self.W, self.bias, self.routing_iterations)\n",
    "    return v_j.unsqueeze(-1)\n",
    "\n",
    "\n",
    "class ReconstructionModule(nn.Module):\n",
    "  def __init__(self, capsule_size=16, num_capsules=10, imsize=28,img_channel=1, batchnorm=False):\n",
    "    super(ReconstructionModule, self).__init__()\n",
    "    \n",
    "    self.num_capsules = num_capsules\n",
    "    self.capsule_size = capsule_size\n",
    "    self.imsize = imsize\n",
    "    self.img_channel = img_channel\n",
    "    if batchnorm:\n",
    "        self.decoder = nn.Sequential(\n",
    "              nn.Linear(capsule_size*num_capsules, 512),\n",
    "              nn.BatchNorm1d(512),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(512, 1024),        \n",
    "              nn.BatchNorm1d(1024),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(1024, imsize*imsize*img_channel),\n",
    "              nn.Sigmoid()\n",
    "        )\n",
    "    else:\n",
    "        self.decoder = nn.Sequential(\n",
    "              nn.Linear(capsule_size*num_capsules, 512),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(512, 1024),        \n",
    "              nn.ReLU(),\n",
    "              nn.Linear(1024, imsize*imsize*img_channel),\n",
    "              nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "  def forward(self, x, target=None):\n",
    "    batch_size = x.size(0)\n",
    "    if target is None:\n",
    "      classes = torch.norm(x, dim=2)\n",
    "      max_length_indices = classes.max(dim=1)[1].squeeze()\n",
    "    else:\n",
    "      max_length_indices = target.max(dim=1)[1]\n",
    "    \n",
    "    masked = Variable(x.new_tensor(torch.eye(self.num_capsules)))\n",
    "    masked = masked.cuda()\n",
    "    masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
    "    decoder_input = (x * masked[:, :, None, None]).view(batch_size, -1)\n",
    "\n",
    "    reconstructions = self.decoder(decoder_input)\n",
    "    reconstructions = reconstructions.view(-1, self.img_channel, self.imsize, self.imsize)\n",
    "    return reconstructions, masked\n",
    "\n",
    "class ConvReconstructionModule(nn.Module):\n",
    "  def __init__(self, num_capsules=10, capsule_size=16, imsize=28,img_channels=1, batchnorm=False):\n",
    "    super(ConvReconstructionModule, self).__init__()\n",
    "    self.num_capsules = num_capsules\n",
    "    self.capsule_size = capsule_size\n",
    "    self.imsize = imsize\n",
    "    self.img_channels = img_channels\n",
    "    self.grid_size = 6\n",
    "    if batchnorm:\n",
    "      self.FC = nn.Sequential(\n",
    "        nn.Linear(capsule_size * num_capsules, num_capsules * (self.grid_size)**2 ),\n",
    "        nn.BatchNorm1d(num_capsules * self.grid_size**2),\n",
    "        nn.ReLU()\n",
    "      )\n",
    "      self.decoder = nn.Sequential(\n",
    "          nn.ConvTranspose2d(in_channels=self.num_capsules, out_channels=32, kernel_size=9, stride=2),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=9, stride=1),\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=2, stride=1),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "    else:\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(capsule_size * num_capsules, num_capsules *(self.grid_size**2) ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.ConvTranspose2d(in_channels=self.num_capsules, out_channels=32, kernel_size=9, stride=2),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=9, stride=1),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=2, stride=1),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "  def forward(self, x, target=None):\n",
    "    batch_size = x.size(0)\n",
    "    if target is None:\n",
    "      classes = torch.norm(x, dim=2)\n",
    "      max_length_indices = classes.max(dim=1)[1].squeeze()\n",
    "    else:\n",
    "      max_length_indices = target.max(dim=1)[1]\n",
    "    \n",
    "    masked = x.new_tensor(torch.eye(self.num_capsules))\n",
    "    masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
    "\n",
    "    decoder_input = (x * masked[:, :, None, None]).view(batch_size, -1)\n",
    "    decoder_input = self.FC(decoder_input)\n",
    "    decoder_input = decoder_input.view(batch_size,self.num_capsules, self.grid_size, self.grid_size)\n",
    "    reconstructions = self.decoder(decoder_input)\n",
    "    reconstructions = reconstructions.view(-1, self.img_channels, self.imsize, self.imsize)\n",
    "    \n",
    "    return reconstructions, masked\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SmallNorbConvReconstructionModule(nn.Module):\n",
    "  def __init__(self, num_capsules=10, capsule_size=16, imsize=28,img_channels=1, batchnorm=False):\n",
    "    super(SmallNorbConvReconstructionModule, self).__init__()\n",
    "    self.num_capsules = num_capsules\n",
    "    self.capsule_size = capsule_size\n",
    "    self.imsize = imsize\n",
    "    self.img_channels = img_channels\n",
    "    \n",
    "    self.grid_size = 4\n",
    "    \n",
    "    if batchnorm:\n",
    "      self.FC = nn.Sequential(\n",
    "            nn.Linear(capsule_size * num_capsules, num_capsules *self.grid_size*self.grid_size),\n",
    "            nn.BatchNorm1d(num_capsules * self.grid_size**2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "      self.decoder = nn.Sequential(\n",
    "          nn.ConvTranspose2d(in_channels=num_capsules, out_channels=32, kernel_size=9, stride=2),\n",
    "          nn.BatchNorm2d(32),            \n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=9, stride=1),\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=9, stride=1),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=2, stride=1),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "    else:\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(capsule_size * num_capsules, num_capsules *(self.grid_size**2) ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.ConvTranspose2d(in_channels=num_capsules, out_channels=32, kernel_size=9, stride=2),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=32, out_channels=64, kernel_size=9, stride=1),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=64, out_channels=128, kernel_size=9, stride=1),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=128, out_channels=img_channels, kernel_size=2, stride=1),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "  def forward(self, x, target=None):\n",
    "    batch_size = x.size(0)\n",
    "    if target is None:\n",
    "      classes = torch.norm(x, dim=2)\n",
    "      max_length_indices = classes.max(dim=1)[1].squeeze()\n",
    "    else:\n",
    "      max_length_indices = target.max(dim=1)[1]\n",
    "    masked = Variable(x.new_tensor(torch.eye(self.num_capsules)))\n",
    "    masked = masked.index_select(dim=0, index=max_length_indices.data)\n",
    "\n",
    "    decoder_input = (x * masked[:, :, None, None]).view(batch_size, -1)\n",
    "    decoder_input = self.FC(decoder_input)\n",
    "    decoder_input = decoder_input.view(batch_size,self.num_capsules, self.grid_size, self.grid_size)\n",
    "    reconstructions = self.decoder(decoder_input)\n",
    "    reconstructions = reconstructions.view(-1, self.img_channels, self.imsize, self.imsize)\n",
    "    \n",
    "    return reconstructions, masked\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "  \n",
    "  def __init__(self,\n",
    "               reconstruction_type = \"FC\",\n",
    "               imsize=28,\n",
    "               num_classes=10,\n",
    "               routing_iterations=3,\n",
    "               primary_caps_gridsize=6,\n",
    "               img_channels = 1,\n",
    "               batchnorm = False,\n",
    "               loss = \"L2\",\n",
    "               num_primary_capsules=32,\n",
    "               leaky_routing = False\n",
    "              ):\n",
    "    super(CapsNet, self).__init__()\n",
    "    self.num_classes = num_classes\n",
    "    if leaky_routing:\n",
    "        num_classes += 1\n",
    "        self.num_classes += 1\n",
    "        \n",
    "    self.imsize=imsize\n",
    "    self.conv_layer = ConvLayer(in_channels=img_channels, batchnorm=batchnorm)\n",
    "    self.leaky_routing = leaky_routing\n",
    "\n",
    "    self.primary_capsules = PrimaryCapules(primary_caps_gridsize=primary_caps_gridsize,\n",
    "                                           batchnorm=batchnorm,\n",
    "                                           num_capsules = num_primary_capsules)\n",
    "    \n",
    "    self.digit_caps = ClassCapsules(num_capsules=num_classes,\n",
    "                                    num_routes=num_primary_capsules*primary_caps_gridsize*primary_caps_gridsize,\n",
    "                                    routing_iterations=routing_iterations,\n",
    "                                    leaky=leaky_routing)\n",
    "\n",
    "    if reconstruction_type == \"FC\":\n",
    "        self.decoder = ReconstructionModule(imsize=imsize,\n",
    "                                            num_capsules=num_classes,\n",
    "                                            img_channel=img_channels, \n",
    "                                            batchnorm=batchnorm)\n",
    "    elif reconstruction_type == \"Conv32\":\n",
    "        self.decoder = SmallNorbConvReconstructionModule(num_capsules=num_classes,\n",
    "                                                         imsize=imsize, \n",
    "                                                         img_channels=img_channels, \n",
    "                                                         batchnorm=batchnorm)            \n",
    "    else:\n",
    "        self.decoder = ConvReconstructionModule(num_capsules=num_classes,\n",
    "                                                imsize=imsize, \n",
    "                                                img_channels=img_channels,\n",
    "                                                batchnorm=batchnorm)\n",
    "    \n",
    "    if loss == \"L2\":\n",
    "        self.reconstruction_criterion = nn.MSELoss(reduction=\"none\")\n",
    "    if loss == \"L1\":\n",
    "        self.reconstruction_criterion = nn.L1Loss(reduction=\"none\")\n",
    "  \n",
    "  def forward(self, x, target=None):\n",
    "    output = self.conv_layer(x)\n",
    "    output = self.primary_capsules(output)\n",
    "    output = self.digit_caps(output)\n",
    "    reconstruction, masked = self.decoder(output, target)\n",
    "\n",
    "    return output, reconstruction, masked\n",
    "  \n",
    "  def loss(self, images, labels, capsule_output,  reconstruction, alpha):\n",
    "    marg_loss = self.margin_loss(capsule_output, labels)\n",
    "    rec_loss = self.reconstruction_loss(images, reconstruction)\n",
    "    total_loss = (marg_loss + alpha * rec_loss).mean()\n",
    "    return total_loss, rec_loss.mean(), marg_loss.mean()\n",
    "  \n",
    "  def margin_loss(self, x, labels):\n",
    "    batch_size = x.size(0)\n",
    "    v_c = torch.norm(x, dim=2, keepdim=True)\n",
    "    \n",
    "    left = functional.relu(0.9 - v_c).view(batch_size, -1) ** 2\n",
    "    right = functional.relu(v_c - 0.1).view(batch_size, -1) ** 2\n",
    "\n",
    "    loss = labels * left + 0.5 *(1-labels)*right\n",
    "    loss = loss.sum(dim=1)\n",
    "    return loss\n",
    "  \n",
    "  def reconstruction_loss(self, data, reconstructions):\n",
    "    batch_size = reconstructions.size(0)\n",
    "    reconstructions = reconstructions.view(batch_size, -1)\n",
    "    data = data.view(batch_size, -1)\n",
    "    loss = self.reconstruction_criterion(reconstructions, data)\n",
    "    loss = loss.sum(dim=1)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dcU0FyV2FQ6Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:141: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:145: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:141: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:145: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-3-215db03644a6>:141: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  target = self.labels[index % 24300] if self.mode is \"all\" else self.labels[index]\n",
      "<ipython-input-3-215db03644a6>:145: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  info = self.infos[index % 24300] if self.mode is \"all\" else self.infos[index]\n"
     ]
    }
   ],
   "source": [
    "# Loader taken from https://github.com/mavanb/vision/blob/448fac0f38cab35a387666d553b9d5e4eec4c5e6/torchvision/datasets/utils.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import errno\n",
    "import struct\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.datasets.utils import download_url, check_integrity\n",
    "\n",
    "\n",
    "class SmallNORB(data.Dataset):\n",
    "    \"\"\"`MNIST <https://cs.nyu.edu/~ylclab/data/norb-v1.0-small//>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where processed folder and\n",
    "            and  raw folder exist.\n",
    "        train (bool, optional): If True, creates dataset from the training files,\n",
    "            otherwise from the test files.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If the dataset is already processed, it is not processed\n",
    "            and downloaded again. If dataset is only already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        info_transform (callable, optional): A function/transform that takes in the\n",
    "            info and transforms it.\n",
    "        mode (string, optional): Denotes how the images in the data files are returned. Possible values:\n",
    "            - all (default): both left and right are included separately.\n",
    "            - stereo: left and right images are included as corresponding pairs.\n",
    "            - left: only the left images are included.\n",
    "            - right: only the right images are included.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_root = \"https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/\"\n",
    "    data_files = {\n",
    "        'train': {\n",
    "            'dat': {\n",
    "                \"name\": 'smallnorb-5x46789x9x18x6x2x96x96-training-dat.mat',\n",
    "                \"md5_gz\": \"66054832f9accfe74a0f4c36a75bc0a2\",\n",
    "                \"md5\": \"8138a0902307b32dfa0025a36dfa45ec\"\n",
    "            },\n",
    "            'info': {\n",
    "                \"name\": 'smallnorb-5x46789x9x18x6x2x96x96-training-info.mat',\n",
    "                \"md5_gz\": \"51dee1210a742582ff607dfd94e332e3\",\n",
    "                \"md5\": \"19faee774120001fc7e17980d6960451\"\n",
    "            },\n",
    "            'cat': {\n",
    "                \"name\": 'smallnorb-5x46789x9x18x6x2x96x96-training-cat.mat',\n",
    "                \"md5_gz\": \"23c8b86101fbf0904a000b43d3ed2fd9\",\n",
    "                \"md5\": \"fd5120d3f770ad57ebe620eb61a0b633\"\n",
    "            },\n",
    "        },\n",
    "        'test': {\n",
    "            'dat': {\n",
    "                \"name\": 'smallnorb-5x01235x9x18x6x2x96x96-testing-dat.mat',\n",
    "                \"md5_gz\": \"e4ad715691ed5a3a5f138751a4ceb071\",\n",
    "                \"md5\": \"e9920b7f7b2869a8f1a12e945b2c166c\"\n",
    "            },\n",
    "            'info': {\n",
    "                \"name\": 'smallnorb-5x01235x9x18x6x2x96x96-testing-info.mat',\n",
    "                \"md5_gz\": \"a9454f3864d7fd4bb3ea7fc3eb84924e\",\n",
    "                \"md5\": \"7c5b871cc69dcadec1bf6a18141f5edc\"\n",
    "            },\n",
    "            'cat': {\n",
    "                \"name\": 'smallnorb-5x01235x9x18x6x2x96x96-testing-cat.mat',\n",
    "                \"md5_gz\": \"5aa791cd7e6016cf957ce9bdb93b8603\",\n",
    "                \"md5\": \"fd5120d3f770ad57ebe620eb61a0b633\"\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    raw_folder = 'raw'\n",
    "    processed_folder = 'processed'\n",
    "    train_image_file = 'train_img'\n",
    "    train_label_file = 'train_label'\n",
    "    train_info_file = 'train_info'\n",
    "    test_image_file = 'test_img'\n",
    "    test_label_file = 'test_label'\n",
    "    test_info_file = 'test_info'\n",
    "    extension = '.pt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, info_transform=None, download=False,\n",
    "                 mode=\"all\"):\n",
    "\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.info_transform = info_transform\n",
    "        self.train = train  # training set or test set\n",
    "        self.mode = mode\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        # load test or train set\n",
    "        image_file = self.train_image_file if self.train else self.test_image_file\n",
    "        label_file = self.train_label_file if self.train else self.test_label_file\n",
    "        info_file = self.train_info_file if self.train else self.test_info_file\n",
    "        # load labels\n",
    "        self.labels = self._load(label_file)\n",
    "        # load info files\n",
    "        self.infos = self._load(info_file)\n",
    "\n",
    "        # load right set\n",
    "        if self.mode == \"left\":\n",
    "            self.data = self._load(\"{}_left\".format(image_file))\n",
    "\n",
    "        # load left set\n",
    "        elif self.mode == \"right\":\n",
    "            self.data = self._load(\"{}_right\".format(image_file))\n",
    "\n",
    "        elif self.mode == \"all\" or self.mode == \"stereo\":\n",
    "            left_data = self._load(\"{}_left\".format(image_file))\n",
    "            right_data = self._load(\"{}_right\".format(image_file))\n",
    "            # load stereo\n",
    "            if self.mode == \"stereo\":\n",
    "                self.data = torch.stack((left_data, right_data), dim=1)\n",
    "    \n",
    "            # load all\n",
    "            else:\n",
    "                self.data = torch.cat((left_data, right_data), dim=0)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            mode ``all'', ``left'', ``right'':\n",
    "                tuple: (image, target, info)\n",
    "            mode ``stereo'':\n",
    "                tuple: (image left, image right, target, info)\n",
    "        \"\"\"\n",
    "        target = self.labels[index % 24300] if self.mode is \"all\" else self.labels[index]\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        info = self.infos[index % 24300] if self.mode is \"all\" else self.infos[index]\n",
    "        if self.info_transform is not None:\n",
    "            info = self.info_transform(info)\n",
    "\n",
    "        if self.mode == \"stereo\":\n",
    "            img_left = self._transform(self.data[index, 0])\n",
    "            img_right = self._transform(self.data[index, 1])\n",
    "            return img_left, img_right, target, info\n",
    "\n",
    "        img = self._transform(self.data[index])\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _transform(self, img):\n",
    "        # doing this so that it is consistent with all other data sets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img.numpy(), mode='L')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def _load(self, file_name):\n",
    "        return torch.load(os.path.join(self.root, self.processed_folder, file_name + self.extension))\n",
    "\n",
    "    def _save(self, file, file_name):\n",
    "        with open(os.path.join(self.root, self.processed_folder, file_name + self.extension), 'wb') as f:\n",
    "            torch.save(file, f)\n",
    "\n",
    "    def _check_exists(self):\n",
    "        \"\"\" Check if processed files exists.\"\"\"\n",
    "        files = (\n",
    "            \"{}_left\".format(self.train_image_file),\n",
    "            \"{}_right\".format(self.train_image_file),\n",
    "            \"{}_left\".format(self.test_image_file),\n",
    "            \"{}_right\".format(self.test_image_file),\n",
    "            self.test_label_file,\n",
    "            self.train_label_file\n",
    "        )\n",
    "        fpaths = [os.path.exists(os.path.join(self.root, self.processed_folder, f + self.extension)) for f in files]\n",
    "        return False not in fpaths\n",
    "\n",
    "    def _flat_data_files(self):\n",
    "        return [j for i in self.data_files.values() for j in list(i.values())]\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        \"\"\"Check if unpacked files have correct md5 sum.\"\"\"\n",
    "        root = self.root\n",
    "        for file_dict in self._flat_data_files():\n",
    "            filename = file_dict[\"name\"]\n",
    "            md5 = file_dict[\"md5\"]\n",
    "            fpath = os.path.join(root, self.raw_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the SmallNORB data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        import gzip\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # check if already extracted and verified\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "        else:\n",
    "            # download and extract\n",
    "            for file_dict in self._flat_data_files():\n",
    "                url = self.dataset_root + file_dict[\"name\"] + '.gz'\n",
    "                filename = file_dict[\"name\"]\n",
    "                gz_filename = filename + '.gz'\n",
    "                md5 = file_dict[\"md5_gz\"]\n",
    "                fpath = os.path.join(self.root, self.raw_folder, filename)\n",
    "                gz_fpath = fpath + '.gz'\n",
    "\n",
    "                # download if compressed file not exists and verified\n",
    "                download_url(url, os.path.join(self.root, self.raw_folder), gz_filename, md5)\n",
    "\n",
    "                print('# Extracting data {}\\n'.format(filename))\n",
    "\n",
    "                with open(fpath, 'wb') as out_f, \\\n",
    "                        gzip.GzipFile(gz_fpath) as zip_f:\n",
    "                    out_f.write(zip_f.read())\n",
    "\n",
    "                os.unlink(gz_fpath)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "\n",
    "        # create processed folder\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # read train files\n",
    "        left_train_img, right_train_img = self._read_image_file(self.data_files[\"train\"][\"dat\"][\"name\"])\n",
    "        train_info = self._read_info_file(self.data_files[\"train\"][\"info\"][\"name\"])\n",
    "        train_label = self._read_label_file(self.data_files[\"train\"][\"cat\"][\"name\"])\n",
    "\n",
    "        # read test files\n",
    "        left_test_img, right_test_img = self._read_image_file(self.data_files[\"test\"][\"dat\"][\"name\"])\n",
    "        test_info = self._read_info_file(self.data_files[\"test\"][\"info\"][\"name\"])\n",
    "        test_label = self._read_label_file(self.data_files[\"test\"][\"cat\"][\"name\"])\n",
    "\n",
    "        # save training files\n",
    "        self._save(left_train_img, \"{}_left\".format(self.train_image_file))\n",
    "        self._save(right_train_img, \"{}_right\".format(self.train_image_file))\n",
    "        self._save(train_label, self.train_label_file)\n",
    "        self._save(train_info, self.train_info_file)\n",
    "\n",
    "        # save test files\n",
    "        self._save(left_test_img, \"{}_left\".format(self.test_image_file))\n",
    "        self._save(right_test_img, \"{}_right\".format(self.test_image_file))\n",
    "        self._save(test_label, self.test_label_file)\n",
    "        self._save(test_info, self.test_info_file)\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_header(file_pointer):\n",
    "        # Read magic number and ignore\n",
    "        struct.unpack('<BBBB', file_pointer.read(4))  # '<' is little endian)\n",
    "\n",
    "        # Read dimensions\n",
    "        dimensions = []\n",
    "        num_dims, = struct.unpack('<i', file_pointer.read(4))  # '<' is little endian)\n",
    "        for _ in range(num_dims):\n",
    "            dimensions.extend(struct.unpack('<i', file_pointer.read(4)))\n",
    "\n",
    "        return dimensions\n",
    "\n",
    "    def _read_image_file(self, file_name):\n",
    "        fpath = os.path.join(self.root, self.raw_folder, file_name)\n",
    "        with open(fpath, mode='rb') as f:\n",
    "            dimensions = self._parse_header(f)\n",
    "            assert dimensions == [24300, 2, 96, 96]\n",
    "            num_samples, _, height, width = dimensions\n",
    "\n",
    "            left_samples = np.zeros(shape=(num_samples, height, width), dtype=np.uint8)\n",
    "            right_samples = np.zeros(shape=(num_samples, height, width), dtype=np.uint8)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "\n",
    "                # left and right images stored in pairs, left first\n",
    "                left_samples[i, :, :] = self._read_image(f, height, width)\n",
    "                right_samples[i, :, :] = self._read_image(f, height, width)\n",
    "\n",
    "        return torch.ByteTensor(left_samples), torch.ByteTensor(right_samples)\n",
    "\n",
    "    @staticmethod\n",
    "    def _read_image(file_pointer, height, width):\n",
    "        \"\"\"Read raw image data and restore shape as appropriate. \"\"\"\n",
    "        image = struct.unpack('<' + height * width * 'B', file_pointer.read(height * width))\n",
    "        image = np.uint8(np.reshape(image, newshape=(height, width)))\n",
    "        return image\n",
    "\n",
    "    def _read_label_file(self, file_name):\n",
    "        fpath = os.path.join(self.root, self.raw_folder, file_name)\n",
    "        with open(fpath, mode='rb') as f:\n",
    "            dimensions = self._parse_header(f)\n",
    "            assert dimensions == [24300]\n",
    "            num_samples = dimensions[0]\n",
    "\n",
    "            struct.unpack('<BBBB', f.read(4))  # ignore this integer\n",
    "            struct.unpack('<BBBB', f.read(4))  # ignore this integer\n",
    "\n",
    "            labels = np.zeros(shape=num_samples, dtype=np.int32)\n",
    "            for i in range(num_samples):\n",
    "                category, = struct.unpack('<i', f.read(4))\n",
    "                labels[i] = category\n",
    "            return torch.LongTensor(labels)\n",
    "\n",
    "    def _read_info_file(self, file_name):\n",
    "        fpath = os.path.join(self.root, self.raw_folder, file_name)\n",
    "        with open(fpath, mode='rb') as f:\n",
    "\n",
    "            dimensions = self._parse_header(f)\n",
    "            assert dimensions == [24300, 4]\n",
    "            num_samples, num_info = dimensions\n",
    "\n",
    "            struct.unpack('<BBBB', f.read(4))  # ignore this integer\n",
    "\n",
    "            infos = np.zeros(shape=(num_samples, num_info), dtype=np.int32)\n",
    "\n",
    "            for r in range(num_samples):\n",
    "                for c in range(num_info):\n",
    "                    info, = struct.unpack('<i', f.read(4))\n",
    "                    infos[r, c] = info\n",
    "\n",
    "        return torch.LongTensor(infos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677,
     "referenced_widgets": [
      "dc8ba0ed3a6e4a73b127e3b4a272e7a6",
      "52584998bc5c484487e9a173c0e14b6f",
      "eec8874ac59840a9824aaa0a7fe12772",
      "055f13e1bf1141feb6da5ab8294906b4",
      "1b5ac685a84b4ae6840b977792adf37a",
      "218e1d5893d9425cb62014d5893a582f",
      "d3ad08fe4b1b423682d411988fbc5011",
      "adf0a77f0419477988faafd9c07594b1",
      "bcb23ab0ac2846c8987780a58e7a3192",
      "937fc54185e845a5878bc2f3d69d64f0",
      "d0716a1dc33043f8a162ddf14663eecc",
      "e44390f1d8194b429b665f9a8cef9a8f",
      "49b4ba446c3a44ea90f42ff457360f86",
      "5488e12dd8d749689598f51b13c20305",
      "cd4ff8f31c844d78b57ffbe1245fd1dc",
      "f6457f440db94f42a8450114254a5ae8",
      "ef44a1e6bc9543d0b6a906111a92c26e",
      "bed00f0137934fae81b5c2d30689dd69",
      "545b4c9eb5be4d5e878456a546ec2841",
      "80bb19a5befd4119a917a39680c11756",
      "46bffa4746754e23951de42c669892ea",
      "923bdee0e01e4c7da18906d6b1e7acae",
      "e02e16206a1346f4906af16ee6d2a592",
      "a0ee81845d474b178618ae88f9d89b20",
      "cd6f0b01b1bc4d51aefed9a1740a5750",
      "68f2c824a01c42758cae4423672f5f61",
      "e7f5c7edd4a2428c84dc5a2e9bc25160",
      "2974d7d01e264e02b283b6a321ca11b8",
      "70e8e5f877e2416090bbd2599c643ebf",
      "2b55d061d2934d1ea19307aa22bfb319",
      "7e616d12c8b84d259ec29b06186c4c7f",
      "174dbc5cde9947c5b89e0c4cfa7183eb",
      "c6d5d8fdaa644a43bc20ca97a2c0904a",
      "1c1f8ff0b46645f5b90b44fd8d53e940",
      "663f0b5f1b26466587729c42307faa20",
      "0cc8396059de4df8827caaf9f06d5956",
      "8a44fd92a3164b0e8aae6c312d65430a",
      "fcab9180e8cc415fbcb53d87d4076a73",
      "2ab8d75372eb4f8f980252b49ebe1e5c",
      "7c5b5fff91884d72988d284107992bdc",
      "d3b98b23a8644360801ff99ddc9165d1",
      "a855e4a5c0ad44f5a46da7a763e214f3",
      "90d83704454f4157acfa85ca9b58474e",
      "88cf156458154caebd6792b323f9de6c",
      "727da93ff3614093b9c639defccf6e19",
      "9c905c34479d4c7b9baa90addbefb76e",
      "a3d3e10daf0d4712ab199aba29096599",
      "91a38b568e3b4eb5857a01af55483774"
     ]
    },
    "id": "FuP1m-yxFVUw",
    "outputId": "3c9b1854-8c19-45af-9d2e-ce947d78bc40"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(48),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ColorJitter(brightness=32./255, contrast=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.704,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(48),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.704,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "train_dataset = SmallNORB('./datasets/smallNORB/', train=True, download=True, transform=train_transform)\n",
    "test_dataset = SmallNORB('./datasets/smallNORB/', train=False, transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "capsnet = CapsNet(reconstruction_type='FC',\n",
    "        imsize=32,\n",
    "        num_classes=5,\n",
    "        routing_iterations = 3,\n",
    "        primary_caps_gridsize=8,\n",
    "        num_primary_capsules=32,\n",
    "        batchnorm=False,\n",
    "        loss = 'L2',\n",
    "        leaky_routing=False)\n",
    "\n",
    "#initialize_weights(capsnet)\n",
    "checkpoint = torch.load('./checkpoint/95.pt')\n",
    "capsnet.load_state_dict(checkpoint['net'])\n",
    "\n",
    "capsnet = capsnet.cuda()\n",
    "\n",
    "best_acc = 0\n",
    "optimizer = torch.optim.Adam(capsnet.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCP9L0QzC7rO",
    "outputId": "45601400-ac40-426f-9e6a-c9ab0be3ab81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "di7Ee8SuIcRf",
    "outputId": "03cd6b1a-ded7-40b2-e167-a57bf5e062a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-3.3871e-02, -3.4372e-02, -2.5176e-02,  ..., -3.3590e-02,\n",
      "           -3.9744e-02, -4.6258e-02],\n",
      "          [-3.7661e-02, -3.2365e-02, -2.4548e-02,  ..., -2.7167e-02,\n",
      "           -3.7441e-02, -4.6642e-02],\n",
      "          [-3.7373e-02, -3.6723e-02, -3.2977e-02,  ..., -2.6947e-02,\n",
      "           -3.5803e-02, -4.5367e-02],\n",
      "          ...,\n",
      "          [-3.7107e-02, -3.7552e-02, -3.6008e-02,  ..., -5.1857e-02,\n",
      "           -5.2370e-02, -3.9731e-02],\n",
      "          [-4.4641e-02, -4.2960e-02, -4.2923e-02,  ..., -4.7699e-02,\n",
      "           -3.9117e-02, -2.4351e-02],\n",
      "          [-5.5089e-02, -5.2823e-02, -5.1550e-02,  ..., -2.7869e-02,\n",
      "           -1.6198e-02, -4.6681e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.3599e-02, -4.1154e-02, -3.8226e-02,  ..., -1.7950e-02,\n",
      "           -1.8593e-02, -1.8396e-02],\n",
      "          [-4.2964e-02, -3.5716e-02, -2.5391e-02,  ..., -4.0317e-02,\n",
      "           -3.8780e-02, -3.8879e-02],\n",
      "          [-3.9582e-02, -3.2994e-02, -2.9308e-02,  ..., -2.4089e-02,\n",
      "           -2.5968e-02, -4.3178e-02],\n",
      "          ...,\n",
      "          [-5.7795e-02, -5.9976e-02, -5.9828e-02,  ..., -5.3113e-02,\n",
      "           -5.4703e-02, -5.5795e-02],\n",
      "          [-4.5995e-02, -5.1877e-02, -5.6770e-02,  ..., -5.4743e-02,\n",
      "           -5.8539e-02, -5.9831e-02],\n",
      "          [-2.3618e-02, -2.5306e-02, -3.9166e-02,  ..., -5.4579e-02,\n",
      "           -5.4650e-02, -3.9306e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.5947e-02, -2.7177e-02, -2.7612e-02,  ..., -2.7210e-02,\n",
      "           -2.6234e-02, -2.4443e-02],\n",
      "          [-2.5555e-02, -2.7258e-02, -2.7483e-02,  ..., -2.8065e-02,\n",
      "           -2.7374e-02, -2.5637e-02],\n",
      "          [-2.6083e-02, -2.7163e-02, -2.7772e-02,  ..., -2.7482e-02,\n",
      "           -2.6945e-02, -2.5752e-02],\n",
      "          ...,\n",
      "          [-2.4288e-02, -2.6364e-02, -2.7721e-02,  ..., -2.7638e-02,\n",
      "           -2.6890e-02, -2.5840e-02],\n",
      "          [-2.2700e-02, -2.4600e-02, -2.6952e-02,  ..., -2.8021e-02,\n",
      "           -2.7913e-02, -2.7175e-02],\n",
      "          [-2.0666e-02, -2.3180e-02, -2.4835e-02,  ..., -2.8456e-02,\n",
      "           -2.7948e-02, -2.7228e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.3984e-03, -3.3944e-02, -6.1687e-02,  ..., -1.3084e-01,\n",
      "           -1.1719e-01, -8.3778e-02],\n",
      "          [-3.4240e-03, -6.1766e-02, -9.1679e-02,  ..., -8.2904e-02,\n",
      "           -1.0881e-01, -5.3003e-02],\n",
      "          [-4.8402e-02, -9.6004e-02, -7.1510e-02,  ..., -1.1553e-01,\n",
      "           -9.4904e-02, -1.7389e-02],\n",
      "          ...,\n",
      "          [-8.1284e-03, -1.2126e-01, -1.1359e-01,  ..., -9.8540e-02,\n",
      "           -4.5350e-02,  2.1020e-02],\n",
      "          [ 4.4172e-02, -4.1506e-02, -1.2541e-01,  ..., -7.6869e-02,\n",
      "           -2.7289e-02,  1.5086e-02],\n",
      "          [ 8.8953e-02,  1.9752e-02, -6.9244e-02,  ..., -8.8134e-02,\n",
      "           -3.7228e-02,  7.1346e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.1856e-05,  8.1987e-05, -8.5563e-05,  ...,  8.8720e-05,\n",
      "            1.5779e-04,  7.7449e-05],\n",
      "          [-3.7578e-04,  7.0788e-05, -8.0681e-05,  ...,  3.4011e-05,\n",
      "            8.3075e-05,  3.4134e-04],\n",
      "          [ 8.9101e-05, -1.9119e-04, -2.4782e-04,  ...,  1.3857e-05,\n",
      "           -2.3557e-04,  2.4547e-04],\n",
      "          ...,\n",
      "          [ 3.7183e-04,  1.7978e-04, -1.7805e-04,  ...,  3.7224e-04,\n",
      "            8.0390e-05,  1.5645e-04],\n",
      "          [-1.6789e-04,  3.2448e-04, -2.5115e-04,  ..., -1.3559e-04,\n",
      "           -1.6560e-05, -2.6905e-05],\n",
      "          [-3.7250e-06, -1.0887e-04, -2.9701e-05,  ..., -5.0685e-05,\n",
      "           -2.4849e-04,  1.0497e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.5881e-02, -3.6030e-02, -3.5925e-02,  ..., -3.3543e-02,\n",
      "           -3.2322e-02, -2.9642e-02],\n",
      "          [-3.1590e-02, -3.2029e-02, -3.1155e-02,  ..., -3.4717e-02,\n",
      "           -3.5475e-02, -3.5523e-02],\n",
      "          [-2.7871e-02, -2.7179e-02, -2.6451e-02,  ..., -2.9120e-02,\n",
      "           -3.1223e-02, -3.2038e-02],\n",
      "          ...,\n",
      "          [-3.5082e-02, -3.2948e-02, -3.0923e-02,  ..., -3.1826e-02,\n",
      "           -3.6031e-02, -3.4621e-02],\n",
      "          [-3.4261e-02, -3.3753e-02, -3.3068e-02,  ..., -3.1555e-02,\n",
      "           -3.5277e-02, -3.0220e-02],\n",
      "          [-2.9747e-02, -3.4494e-02, -3.5336e-02,  ..., -3.2247e-02,\n",
      "           -3.2136e-02, -2.5210e-02]]]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(capsnet.conv_layer.conv[0].weight)\n",
    "capsnet.conv_layer.conv[0].weight = nn.Parameter(quantize(capsnet.conv_layer.conv[0].weight,8,7))\n",
    "capsnet.conv_layer.conv[0].bias = nn.Parameter(quantize(capsnet.conv_layer.conv[0].bias,8,7))\n",
    "capsnet.primary_capsules.capsules[0][0].weight = nn.Parameter(quantize(capsnet.primary_capsules.capsules[0][0].weight,8,7))\n",
    "capsnet.primary_capsules.capsules[0][0].bias   = nn.Parameter(quantize(capsnet.primary_capsules.capsules[0][0].bias,8,7))\n",
    "capsnet.primary_capsules.capsules[1][0].weight = nn.Parameter(quantize(capsnet.primary_capsules.capsules[1][0].weight,8,7))\n",
    "capsnet.primary_capsules.capsules[1][0].bias   = nn.Parameter(quantize(capsnet.primary_capsules.capsules[1][0].bias,8,7))\n",
    "capsnet.primary_capsules.capsules[2][0].weight = nn.Parameter(quantize(capsnet.primary_capsules.capsules[2][0].weight,8,7))\n",
    "capsnet.primary_capsules.capsules[2][0].bias   = nn.Parameter(quantize(capsnet.primary_capsules.capsules[2][0].bias,8,7))\n",
    "capsnet.primary_capsules.capsules[3][0].weight = nn.Parameter(quantize(capsnet.primary_capsules.capsules[3][0].weight,8,7))\n",
    "capsnet.primary_capsules.capsules[3][0].bias   = nn.Parameter(quantize(capsnet.primary_capsules.capsules[3][0].bias,8,7))\n",
    "capsnet.primary_capsules.capsules[4][0].weight = nn.Parameter(quantize(capsnet.primary_capsules.capsules[4][0].weight,8,7))\n",
    "capsnet.primary_capsules.capsules[4][0].bias   = nn.Parameter(quantize(capsnet.primary_capsules.capsules[4][0].bias,8,7))\n",
    "capsnet.primary_capsules.capsules[5][0].weight = nn.Parameter(quantize(capsnet.primary_capsules.capsules[5][0].weight,8,7))\n",
    "capsnet.primary_capsules.capsules[5][0].bias   = nn.Parameter(quantize(capsnet.primary_capsules.capsules[5][0].bias,8,7))\n",
    "capsnet.primary_capsules.capsules[6][0].weight = nn.Parameter(quantize(capsnet.primary_capsules.capsules[6][0].weight,8,7))\n",
    "capsnet.primary_capsules.capsules[6][0].bias   = nn.Parameter(quantize(capsnet.primary_capsules.capsules[6][0].bias,8,7))\n",
    "capsnet.primary_capsules.capsules[7][0].weight = nn.Parameter(quantize(capsnet.primary_capsules.capsules[7][0].weight,8,7))\n",
    "capsnet.primary_capsules.capsules[7][0].bias   = nn.Parameter(quantize(capsnet.primary_capsules.capsules[7][0].bias,8,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Geg7x7pwNqcD",
    "outputId": "de090369-9f97-4e65-d501-76928e284a4d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "PPE-YW5qDbGC",
    "outputId": "900ed068-65e8-43cd-a2c4-ce7df70c9239"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test   0:   0%|                                                                                | 0/380 [00:00<?, ?it/s]<ipython-input-2-736a1c1b634e>:186: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  masked = Variable(x.new_tensor(torch.eye(self.num_capsules)))\n",
      "Test   0: 100%|######################################################################| 380/380 [00:20<00:00, 18.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.8473868312757201%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test   1: 100%|######################################################################| 380/380 [00:20<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.8473868312757201%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7a78c2535109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-7a78c2535109>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascii\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Test {:3d}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-215db03644a6>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_right\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-215db03644a6>\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \"\"\"\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0moh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\sangjun\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1941\u001b[0m                 )\n\u001b[0;32m   1942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1943\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1945\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    capsnet.train()\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    for batch, (data, target) in tqdm(list(enumerate(train_loader)), ascii=True, desc=\"Epoch{:3d}\".format(epoch)):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        target = torch.eye(5).cuda().index_select(dim=0, index=target)\n",
    "        optimizer.zero_grad()\n",
    "        capsule_output, reconstructions, _ = capsnet(data, target)\n",
    "        predictions = torch.norm(capsule_output.squeeze(), dim=2)\n",
    "        loss, rec_loss, marg_loss = capsnet.loss(data, target, capsule_output, reconstructions, 0.0005)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_correct += (target.max(dim=1)[1] == predictions.max(dim=1)[1]).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    print(\"acc = {}%\".format(train_correct/total))\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    capsnet.eval()\n",
    "    test_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_id, (data, target) in tqdm(list(enumerate(test_loader)), ascii=True, desc=\"Test {:3d}\".format(epoch)):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        target = torch.eye(5).cuda().index_select(dim=0, index=target)\n",
    "\n",
    "        capsule_output, reconstructions, predictions = capsnet(data)\n",
    "        data = denormalize(data)\n",
    "        loss, rec_loss, marg_loss = capsnet.loss(data, target, capsule_output, reconstructions, 0.0005)\n",
    "\n",
    "        test_correct += (target.max(dim=1)[1] == predictions.max(dim=1)[1]).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    print(\"acc = {}%\".format(test_correct/total))\n",
    "    acc = 100.*test_correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': capsnet.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/{}.pt'.format(str(acc)))\n",
    "        best_acc = acc\n",
    "\n",
    "for epoch in range(200):\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "smallNORB0526 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "055f13e1bf1141feb6da5ab8294906b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adf0a77f0419477988faafd9c07594b1",
      "placeholder": "​",
      "style": "IPY_MODEL_d3ad08fe4b1b423682d411988fbc5011",
      "value": " 131896320/? [00:11&lt;00:00, 11005168.16it/s]"
     }
    },
    "0cc8396059de4df8827caaf9f06d5956": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c5b5fff91884d72988d284107992bdc",
      "placeholder": "​",
      "style": "IPY_MODEL_2ab8d75372eb4f8f980252b49ebe1e5c",
      "value": " 11264/? [00:57&lt;00:00, 196.93it/s]"
     }
    },
    "174dbc5cde9947c5b89e0c4cfa7183eb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b5ac685a84b4ae6840b977792adf37a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1c1f8ff0b46645f5b90b44fd8d53e940": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "218e1d5893d9425cb62014d5893a582f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2974d7d01e264e02b283b6a321ca11b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_174dbc5cde9947c5b89e0c4cfa7183eb",
      "placeholder": "​",
      "style": "IPY_MODEL_7e616d12c8b84d259ec29b06186c4c7f",
      "value": " 130800640/? [00:05&lt;00:00, 23481063.43it/s]"
     }
    },
    "2ab8d75372eb4f8f980252b49ebe1e5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b55d061d2934d1ea19307aa22bfb319": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46bffa4746754e23951de42c669892ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "49b4ba446c3a44ea90f42ff457360f86": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "52584998bc5c484487e9a173c0e14b6f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "545b4c9eb5be4d5e878456a546ec2841": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_923bdee0e01e4c7da18906d6b1e7acae",
      "max": 348,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46bffa4746754e23951de42c669892ea",
      "value": 348
     }
    },
    "5488e12dd8d749689598f51b13c20305": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "663f0b5f1b26466587729c42307faa20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fcab9180e8cc415fbcb53d87d4076a73",
      "max": 10428,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a44fd92a3164b0e8aae6c312d65430a",
      "value": 10428
     }
    },
    "68f2c824a01c42758cae4423672f5f61": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70e8e5f877e2416090bbd2599c643ebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "727da93ff3614093b9c639defccf6e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7c5b5fff91884d72988d284107992bdc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e616d12c8b84d259ec29b06186c4c7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80bb19a5befd4119a917a39680c11756": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0ee81845d474b178618ae88f9d89b20",
      "placeholder": "​",
      "style": "IPY_MODEL_e02e16206a1346f4906af16ee6d2a592",
      "value": " 1024/? [00:00&lt;00:00, 1738.95it/s]"
     }
    },
    "88cf156458154caebd6792b323f9de6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91a38b568e3b4eb5857a01af55483774",
      "placeholder": "​",
      "style": "IPY_MODEL_a3d3e10daf0d4712ab199aba29096599",
      "value": " 1024/? [00:56&lt;00:00, 18.09it/s]"
     }
    },
    "8a44fd92a3164b0e8aae6c312d65430a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "90d83704454f4157acfa85ca9b58474e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c905c34479d4c7b9baa90addbefb76e",
      "max": 347,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_727da93ff3614093b9c639defccf6e19",
      "value": 347
     }
    },
    "91a38b568e3b4eb5857a01af55483774": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "923bdee0e01e4c7da18906d6b1e7acae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "937fc54185e845a5878bc2f3d69d64f0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c905c34479d4c7b9baa90addbefb76e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ee81845d474b178618ae88f9d89b20": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3d3e10daf0d4712ab199aba29096599": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a855e4a5c0ad44f5a46da7a763e214f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adf0a77f0419477988faafd9c07594b1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcb23ab0ac2846c8987780a58e7a3192": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d0716a1dc33043f8a162ddf14663eecc",
       "IPY_MODEL_e44390f1d8194b429b665f9a8cef9a8f"
      ],
      "layout": "IPY_MODEL_937fc54185e845a5878bc2f3d69d64f0"
     }
    },
    "bed00f0137934fae81b5c2d30689dd69": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6d5d8fdaa644a43bc20ca97a2c0904a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_663f0b5f1b26466587729c42307faa20",
       "IPY_MODEL_0cc8396059de4df8827caaf9f06d5956"
      ],
      "layout": "IPY_MODEL_1c1f8ff0b46645f5b90b44fd8d53e940"
     }
    },
    "cd4ff8f31c844d78b57ffbe1245fd1dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd6f0b01b1bc4d51aefed9a1740a5750": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e7f5c7edd4a2428c84dc5a2e9bc25160",
       "IPY_MODEL_2974d7d01e264e02b283b6a321ca11b8"
      ],
      "layout": "IPY_MODEL_68f2c824a01c42758cae4423672f5f61"
     }
    },
    "d0716a1dc33043f8a162ddf14663eecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5488e12dd8d749689598f51b13c20305",
      "max": 71052,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49b4ba446c3a44ea90f42ff457360f86",
      "value": 71052
     }
    },
    "d3ad08fe4b1b423682d411988fbc5011": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3b98b23a8644360801ff99ddc9165d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_90d83704454f4157acfa85ca9b58474e",
       "IPY_MODEL_88cf156458154caebd6792b323f9de6c"
      ],
      "layout": "IPY_MODEL_a855e4a5c0ad44f5a46da7a763e214f3"
     }
    },
    "dc8ba0ed3a6e4a73b127e3b4a272e7a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eec8874ac59840a9824aaa0a7fe12772",
       "IPY_MODEL_055f13e1bf1141feb6da5ab8294906b4"
      ],
      "layout": "IPY_MODEL_52584998bc5c484487e9a173c0e14b6f"
     }
    },
    "e02e16206a1346f4906af16ee6d2a592": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e44390f1d8194b429b665f9a8cef9a8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6457f440db94f42a8450114254a5ae8",
      "placeholder": "​",
      "style": "IPY_MODEL_cd4ff8f31c844d78b57ffbe1245fd1dc",
      "value": " 71680/? [00:01&lt;00:00, 53552.52it/s]"
     }
    },
    "e7f5c7edd4a2428c84dc5a2e9bc25160": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b55d061d2934d1ea19307aa22bfb319",
      "max": 130799817,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70e8e5f877e2416090bbd2599c643ebf",
      "value": 130799817
     }
    },
    "eec8874ac59840a9824aaa0a7fe12772": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_218e1d5893d9425cb62014d5893a582f",
      "max": 131896188,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b5ac685a84b4ae6840b977792adf37a",
      "value": 131896188
     }
    },
    "ef44a1e6bc9543d0b6a906111a92c26e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_545b4c9eb5be4d5e878456a546ec2841",
       "IPY_MODEL_80bb19a5befd4119a917a39680c11756"
      ],
      "layout": "IPY_MODEL_bed00f0137934fae81b5c2d30689dd69"
     }
    },
    "f6457f440db94f42a8450114254a5ae8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcab9180e8cc415fbcb53d87d4076a73": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
